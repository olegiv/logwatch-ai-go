# LLM Provider Selection
# Options: "anthropic" (default), "ollama", or "lmstudio"
LLM_PROVIDER=anthropic

# Anthropic/Claude Configuration (used when LLM_PROVIDER=anthropic)
ANTHROPIC_API_KEY=sk-ant-xxxxx
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# Ollama Configuration (used when LLM_PROVIDER=ollama)
# Requires Ollama running locally: https://ollama.ai
# Recommended models for Mac M3 Ultra (96GB):
#   - llama3.3:latest (best quality, ~40GB RAM)
#   - qwen2.5:72b (excellent for technical analysis)
#   - deepseek-coder-v2:33b (faster, good quality)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.3:latest

# LM Studio Configuration (used when LLM_PROVIDER=lmstudio)
# Requires LM Studio running locally: https://lmstudio.ai
# Start LM Studio, load a model, and enable "Local Server" mode
# Default model identifier is "local-model" which uses the currently loaded model
LMSTUDIO_BASE_URL=http://localhost:1234
LMSTUDIO_MODEL=local-model

# AI Settings (applies to all providers)
AI_TIMEOUT_SECONDS=120
AI_MAX_TOKENS=8000

# Telegram Notifications
TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_CHANNEL_ARCHIVE_ID=-1001234567890
TELEGRAM_CHANNEL_ALERTS_ID=-1009876543210

# Log Source Configuration
# Options: "logwatch" (default) or "drupal_watchdog"
LOG_SOURCE_TYPE=logwatch

# Logwatch Configuration (used when LOG_SOURCE_TYPE=logwatch)
LOGWATCH_OUTPUT_PATH=/tmp/logwatch-output.txt

# Drupal Watchdog Configuration (used when LOG_SOURCE_TYPE=drupal_watchdog)
# Configure Drupal sites in drupal-sites.json (see configs/drupal-sites.json.example)
# Use: -drupal-site <site_id> to select a site
# Use: -list-drupal-sites to list available sites

# Common Log Settings
MAX_LOG_SIZE_MB=10

# Application Settings
LOG_LEVEL=info
ENABLE_DATABASE=true
DATABASE_PATH=./data/summaries.db

# Preprocessing (for large log files)
ENABLE_PREPROCESSING=true
MAX_PREPROCESSING_TOKENS=150000

# Network Proxy (optional)
HTTP_PROXY=
HTTPS_PROXY=
